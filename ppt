## Updated Architecture with Python & BigQuery
Your pipeline will look like this, with the audit log service now writing structured data directly into a BigQuery table.

User Access (IAP): An authorized Ops team member logs in through the secure URL provided by Identity-Aware Proxy (IAP).

File Upload (Cloud Storage): The user uploads the mapping file (Excel/CSV) or an existing PCD file. Your Python application directs this file to be saved in a Cloud Storage bucket for inputs.

Processing (Cloud Run): The file upload triggers your Python application running on Cloud Run. The code fetches the file from Cloud Storage and performs all the required business logic: parsing, validation, margin calculation, and generation of the output PCD file.

Audit Logging (BigQuery): As the application processes the data, it streams structured log entries for every key action (file received, validation success/fail, changes made) into a predefined table in BigQuery.

Output Generation (Cloud Storage): The newly generated PCD file and a validation summary report are saved to the output Cloud Storage bucket.

Review & Deployment: The Governance team reviews the output files and the validation report. They can also query the BigQuery audit log for a complete, immutable record of the changes before the final file is uploaded to the bank's core FX system.
